{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение с подкреплением Q Learning Taxi\n",
    "\n",
    "**Пространство действий**\n",
    "\n",
    "Существует 6 дискретных детерминированных действий -двигаться на юг,двигаться на север, двигаться на запад, двигаться на юг , забрать пассажира, высадить пассажира\n",
    "\n",
    "\n",
    "**Пространство состояний**\n",
    "\n",
    "Существует 500 дискретных состояний, поскольку имеется 25 позиций такси, 5 возможных местоположений пассажира (включая случай, когда пассажир находится в такси) и 4 местоположения назначения.\n",
    "\n",
    "Обратите внимание, что существует 400 состояний, которые фактически могут быть достигнуты во время эпизода. Отсутствующие состояния соответствуют ситуациям, в которых пассажир находится в том же месте, что и пункт назначения, поскольку это обычно сигнализирует об окончании эпизода. Четыре дополнительных состояния можно наблюдать сразу после успешных эпизодов, когда и пассажир, и такси находятся в пункте назначения. Это дает в общей сложности 404 достижимых дискретных состояния.\n",
    "\n",
    "Каждое пространство состояний представлено кортежем: (taxi_row, taxi_col, passenger_location, destination)\n",
    "\n",
    "Наблюдение - это целое число, которое кодирует соответствующее состояние. Затем кортеж состояний может быть декодирован с помощью метода “decode”.\n",
    "\n",
    "Места расположения пассажиров:\n",
    "\n",
    "0. R(ed)\n",
    "\n",
    "1. G(reen)\n",
    "\n",
    "2. Y(ellow)\n",
    "\n",
    "3. B(lue)\n",
    "\n",
    "4. В такси\n",
    "\n",
    "Destinations:\n",
    "\n",
    "0. R(ed)\n",
    "\n",
    "1. G(reen)\n",
    "\n",
    "2. Y(ellow)\n",
    "\n",
    "3. B(lue)\n",
    "\n",
    "**Награды**\n",
    "\n",
    "-1 за шаг если не случаются другие награды.\n",
    "\n",
    "+20 доставка пассажира.\n",
    "\n",
    "-10 неправильное выполнение действий по “подбору” и “высадке”.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f65802c9250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Параметры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тренировочные параметры\n",
    "n_training_episodes = 10000  # Тренировочных эпизодов\n",
    "learning_rate = 0.7          # Learning rate\n",
    "\n",
    "# Параметры тестирования\n",
    "n_eval_episodes = 10000        # Тестовых эпизодов\n",
    "\n",
    "# Параметры среды\n",
    "env_id = \"Taxi-v3\"     # Имя среды\n",
    "max_steps = 99               # Максимальное число шагов в эпизоде\n",
    "gamma = 0.95                 # Discounting rate\n",
    "eval_seed = []               # The evaluation seed of the environment\n",
    "\n",
    "# Параметры исследования среды\n",
    "max_epsilon = 0.8             # Вероятность исследований на старте\n",
    "min_epsilon = 0.05            # Минимальная вероятность исследований \n",
    "decay_rate = 0.0005            # Экспоненциальная скорость затухания для задач исследования"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  action = np.argmax(Qtable[state][:])\n",
    "  \n",
    "  return action\n",
    "\n",
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  random_int = random.uniform(0,1)\n",
    "  if random_int > epsilon:\n",
    "    action = greedy_policy(Qtable, state)\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "  return action\n",
    "\n",
    "# Создает Qtable размерности (state_space, action_space) и инициализируем  0 \n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable = np.zeros((state_space,action_space))\n",
    "  return Qtable\n",
    "\n",
    "#Запись видео\n",
    "def record_video(env, Qtable, out_directory, fps=1):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param Qtable: Qtable of our agent\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  images = []  \n",
    "  terminated = False\n",
    "  truncated = False\n",
    "  state, info = env.reset(seed=random.randint(0,500))\n",
    "  img = env.render()\n",
    "  images.append(img)\n",
    "  while not terminated or truncated:\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "    state, reward, terminated, truncated, info = env.step(action) \n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
    "\n",
    "# Тестирует агента\n",
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param env: The evaluation environment\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "  :param Q: The Q-table\n",
    "  :param seed: The evaluation seed array \n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in tqdm(range(n_eval_episodes)):\n",
    "    if seed:\n",
    "      state, info = env.reset(seed=seed[episode])\n",
    "    else:\n",
    "      state, info = env.reset()\n",
    "    step = 0\n",
    "    truncated = False\n",
    "    terminated = False\n",
    "    total_rewards_ep = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "      \n",
    "      action = greedy_policy(Q, state)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "        \n",
    "      if terminated or truncated:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward\n",
    "\n",
    "# Обучает  агента\n",
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # Сброс среды\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "      # Выбор действия\n",
    "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      # Обновление Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])   \n",
    "\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "      \n",
    "      state = new_state\n",
    "  return Qtable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициализация среды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Discrete(500)\n",
      "Action space: Discrete(6)\n",
      "Initial state: (368, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n"
     ]
    }
   ],
   "source": [
    "env=gym.make(env_id, render_mode=\"rgb_array\")\n",
    "n_episodes = 10000\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "initial_state = env.reset()\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Initial state: {initial_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  500  possible states\n",
      "There are  6  possible actions\n"
     ]
    }
   ],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roman/projects/OTUS_ADV_HW7/venv/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for _ in range(200):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем Qtable размерности (state_space, action_space) и инициализируем  0 \n",
    "Qtable_taxi = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели --Q-LEARNING ALGORITHM--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:10<00:00, 953.38it/s]\n"
     ]
    }
   ],
   "source": [
    "Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2513.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_reward=7.92 +/- 2.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our Agent\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_taxi, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_reward=7.92 +/- 2.58\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Демонстрация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "record_video(env,Qtable_taxi,'result3.gif')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"result3.gif\" width=\"750\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
